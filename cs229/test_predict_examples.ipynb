{
 "metadata": {
  "name": "",
  "signature": "sha256:d516ed8a172cee829fbe0e2198f784fb00347070c711861934010bb495da9beb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import svm\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.lda import LDA\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.feature_selection import SelectKBest, chi2\n",
      "from sklearn import cross_validation\n",
      "import pandas as pd\n",
      "import compute_stats as cs\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.gridspec as Gridspec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Import the test data\n",
      "test_data = pd.read_csv('test.csv')\n",
      "test_data = test_data.drop('Unnamed: 0',1)\n",
      "test_labels = test_data['label']\n",
      "test_labels[test_labels==1] = 0; test_labels[test_labels>1] = 1\n",
      "test_data = test_data.drop('label',1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import the training data\n",
      "train_data = pd.read_csv('training.csv')\n",
      "train_data = train_data.drop('Unnamed: 0',1)\n",
      "train_labels = train_data['label']\n",
      "train_labels[train_labels==1] = 0; train_labels[train_labels>1] = 1\n",
      "train_data = train_data.drop('label',1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Convert the test and training data into integers for multinomialNB\n",
      "interval = 350\n",
      "int_test = cs.convert_to_integers(pd.DataFrame.copy(test_data),interval)\n",
      "int_train = cs.convert_to_integers(pd.DataFrame.copy(train_data),interval)\n",
      "drop = True\n",
      "if drop:\n",
      "    int_train = int_train.drop('intensity_r',1)\n",
      "    int_train = int_train.drop('percentile',1)\n",
      "    int_train = int_train.drop('reg_ratio',1)\n",
      "    int_test = int_test.drop('intensity_r',1)\n",
      "    int_test = int_test.drop('percentile',1)\n",
      "    int_test = int_test.drop('reg_ratio',1)\n",
      "    train_data = train_data.drop('intensity_r',1)\n",
      "    train_data = train_data.drop('percentile',1)\n",
      "    train_data = train_data.drop('reg_ratio',1)\n",
      "    test_data = test_data.drop('intensity_r',1)\n",
      "    test_data = test_data.drop('percentile',1)\n",
      "    test_data = test_data.drop('reg_ratio',1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_proj = MultinomialNB()\n",
      "log_reg = LogisticRegression(C=1)\n",
      "svm_proj = svm.SVC(C=1)\n",
      "ran_forest = RandomForestClassifier(n_estimators=45)\n",
      "num_algo = 4\n",
      "\n",
      "num_training = np.arange(50,1050,50)\n",
      "\n",
      "scores = np.zeros((num_algo,len(num_training)))\n",
      "i = 0\n",
      "for num in num_training:\n",
      "    # Fit to training data and test on test data for MNB\n",
      "    scorings_nb = cross_validation.cross_val_score(clf_proj,int_train[0:num+1],train_labels[0:num+1],cv=10)\n",
      "    mult_nb_score = np.mean(scorings_nb)\n",
      "    \n",
      "    # Fit for Log-Reg\n",
      "    scorings_log = cross_validation.cross_val_score(log_reg,train_data[0:num+1],train_labels[0:num+1],cv=10)\n",
      "    log_reg_score = np.mean(scorings_log)\n",
      "    \n",
      "    # Fit for SVM\n",
      "    scorings_svm = cross_validation.cross_val_score(svm_proj,train_data[0:num+1],train_labels[0:num+1],cv=10)\n",
      "    svm_proj_score = np.mean(scorings_svm)\n",
      "\n",
      "    # Fit for random forest\n",
      "    scorings_rf = cross_validation.cross_val_score(ran_forest,train_data[0:num+1],train_labels[0:num+1],cv=10)\n",
      "    ran_forest_score = np.mean(scorings_rf)\n",
      "    \n",
      "    scores[:,i] = np.array([mult_nb_score,log_reg_score,svm_proj_score,ran_forest_score])\n",
      "    i = i + 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = pd.DataFrame(scores,columns=num_training,index=['Multinomial Naive Bayes','Linear Logistic Regression','SVM','Random Forest'])\n",
      "scores = 1 - scores\n",
      "scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>50</th>\n",
        "      <th>100</th>\n",
        "      <th>150</th>\n",
        "      <th>200</th>\n",
        "      <th>250</th>\n",
        "      <th>300</th>\n",
        "      <th>350</th>\n",
        "      <th>400</th>\n",
        "      <th>450</th>\n",
        "      <th>500</th>\n",
        "      <th>550</th>\n",
        "      <th>600</th>\n",
        "      <th>650</th>\n",
        "      <th>700</th>\n",
        "      <th>750</th>\n",
        "      <th>800</th>\n",
        "      <th>850</th>\n",
        "      <th>900</th>\n",
        "      <th>950</th>\n",
        "      <th>1000</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>Multinomial Naive Bayes</th>\n",
        "      <td> 0.080000</td>\n",
        "      <td> 0.09</td>\n",
        "      <td> 0.112917</td>\n",
        "      <td> 0.099762</td>\n",
        "      <td> 0.103538</td>\n",
        "      <td> 0.109462</td>\n",
        "      <td> 0.094048</td>\n",
        "      <td> 0.097256</td>\n",
        "      <td> 0.086522</td>\n",
        "      <td> 0.089882</td>\n",
        "      <td> 0.088929</td>\n",
        "      <td> 0.084809</td>\n",
        "      <td> 0.089138</td>\n",
        "      <td> 0.087002</td>\n",
        "      <td> 0.087877</td>\n",
        "      <td> 0.094830</td>\n",
        "      <td> 0.091669</td>\n",
        "      <td> 0.089890</td>\n",
        "      <td> 0.091502</td>\n",
        "      <td> 0.094</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Linear Logistic Regression</th>\n",
        "      <td> 0.296667</td>\n",
        "      <td> 0.22</td>\n",
        "      <td> 0.198750</td>\n",
        "      <td> 0.204286</td>\n",
        "      <td> 0.170923</td>\n",
        "      <td> 0.159247</td>\n",
        "      <td> 0.142381</td>\n",
        "      <td> 0.147012</td>\n",
        "      <td> 0.130870</td>\n",
        "      <td> 0.125804</td>\n",
        "      <td> 0.123377</td>\n",
        "      <td> 0.118197</td>\n",
        "      <td> 0.113706</td>\n",
        "      <td> 0.114105</td>\n",
        "      <td> 0.110509</td>\n",
        "      <td> 0.116065</td>\n",
        "      <td> 0.111601</td>\n",
        "      <td> 0.107656</td>\n",
        "      <td> 0.107281</td>\n",
        "      <td> 0.106</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>SVM</th>\n",
        "      <td> 0.200000</td>\n",
        "      <td> 0.17</td>\n",
        "      <td> 0.225417</td>\n",
        "      <td> 0.203810</td>\n",
        "      <td> 0.190769</td>\n",
        "      <td> 0.175699</td>\n",
        "      <td> 0.159444</td>\n",
        "      <td> 0.157012</td>\n",
        "      <td> 0.144203</td>\n",
        "      <td> 0.139843</td>\n",
        "      <td> 0.141558</td>\n",
        "      <td> 0.134836</td>\n",
        "      <td> 0.132098</td>\n",
        "      <td> 0.131247</td>\n",
        "      <td> 0.131789</td>\n",
        "      <td> 0.134784</td>\n",
        "      <td> 0.129220</td>\n",
        "      <td> 0.124335</td>\n",
        "      <td> 0.124112</td>\n",
        "      <td> 0.123</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Random Forest</th>\n",
        "      <td> 0.136667</td>\n",
        "      <td> 0.10</td>\n",
        "      <td> 0.119583</td>\n",
        "      <td> 0.064762</td>\n",
        "      <td> 0.047692</td>\n",
        "      <td> 0.056237</td>\n",
        "      <td> 0.048492</td>\n",
        "      <td> 0.057256</td>\n",
        "      <td> 0.046570</td>\n",
        "      <td> 0.055882</td>\n",
        "      <td> 0.056266</td>\n",
        "      <td> 0.059891</td>\n",
        "      <td> 0.055268</td>\n",
        "      <td> 0.052797</td>\n",
        "      <td> 0.057246</td>\n",
        "      <td> 0.062407</td>\n",
        "      <td> 0.049384</td>\n",
        "      <td> 0.056581</td>\n",
        "      <td> 0.055735</td>\n",
        "      <td> 0.057</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 113,
       "text": [
        "                                50    100       150       200       250   \\\n",
        "Multinomial Naive Bayes     0.080000  0.09  0.112917  0.099762  0.103538   \n",
        "Linear Logistic Regression  0.296667  0.22  0.198750  0.204286  0.170923   \n",
        "SVM                         0.200000  0.17  0.225417  0.203810  0.190769   \n",
        "Random Forest               0.136667  0.10  0.119583  0.064762  0.047692   \n",
        "\n",
        "                                300       350       400       450       500   \\\n",
        "Multinomial Naive Bayes     0.109462  0.094048  0.097256  0.086522  0.089882   \n",
        "Linear Logistic Regression  0.159247  0.142381  0.147012  0.130870  0.125804   \n",
        "SVM                         0.175699  0.159444  0.157012  0.144203  0.139843   \n",
        "Random Forest               0.056237  0.048492  0.057256  0.046570  0.055882   \n",
        "\n",
        "                                550       600       650       700       750   \\\n",
        "Multinomial Naive Bayes     0.088929  0.084809  0.089138  0.087002  0.087877   \n",
        "Linear Logistic Regression  0.123377  0.118197  0.113706  0.114105  0.110509   \n",
        "SVM                         0.141558  0.134836  0.132098  0.131247  0.131789   \n",
        "Random Forest               0.056266  0.059891  0.055268  0.052797  0.057246   \n",
        "\n",
        "                                800       850       900       950    1000  \n",
        "Multinomial Naive Bayes     0.094830  0.091669  0.089890  0.091502  0.094  \n",
        "Linear Logistic Regression  0.116065  0.111601  0.107656  0.107281  0.106  \n",
        "SVM                         0.134784  0.129220  0.124335  0.124112  0.123  \n",
        "Random Forest               0.062407  0.049384  0.056581  0.055735  0.057  "
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(10,8))\n",
      "fig.set_facecolor('white')\n",
      "fs=17\n",
      "plt.title('Cross-Validated Learning Curves After Model Updates on Test Set',fontsize=fs)\n",
      "plt.plot(num_training,scores.ix['Multinomial Naive Bayes'],'b--',label='Multinomial Naive Bayes')\n",
      "plt.plot(num_training,scores.ix['Linear Logistic Regression'],'r--',label='Linear Logistic Regression')\n",
      "plt.plot(num_training,scores.ix['SVM'],'k--',label='SVM')\n",
      "plt.plot(num_training,scores.ix['Random Forest'],'g--',label='Random Forest')\n",
      "plt.xlabel('Test Set Size',fontsize=fs); plt.ylabel('Generalization Error',fontsize=fs);\n",
      "plt.legend(loc=1,prop={'size':fs-5})\n",
      "plt.show()\n",
      "# When the features are dropped, much better prediction"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot a learning curve for each method for a set of test trials, 200,400,600,800,1000\n",
      "# Plot distributions for key features for blends and non-blends\n",
      "# Plot correlation plots for blends and non-blends\n",
      "# Plot clustered plot for features on blends and non-blends\n",
      "# Plot gradient descent bias on SNR images\n",
      "# Do CV and comparison plot with full feature space"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 197
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Combining the training and test data set\n",
      "whole_data = pd.concat([train_data,test_data])\n",
      "# Gather the unblended and blended data for analysis\n",
      "whole_labels = pd.concat([train_labels,test_labels])\n",
      "blended_labels = whole_labels==1\n",
      "unblended_labels = whole_labels==0\n",
      "blended_data = pd.DataFrame(whole_data.values[blended_labels.values,:],columns=whole_data.columns)\n",
      "unblended_data = pd.DataFrame(whole_data.values[unblended_labels.values,:],columns=whole_data.columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def histogram(whole_data,fs,prop_fs,item,title,xlabel,bin_factor,loc,pos1,pos2,gmm=True):\n",
      "    global fig\n",
      "    bin_size = (max(whole_data[item]) - min(whole_data[item]))/bin_factor\n",
      "    bin_array = np.linspace(min(whole_data[item]),max(whole_data[item]),bin_size+1)                                 \n",
      "    ax1 = fig.add_subplot(gs[pos1,pos2])\n",
      "    blend_hist = np.histogram(whole_data[item][whole_labels.values==1],bins=bin_array)\n",
      "    unblend_hist = np.histogram(whole_data[item][whole_labels.values==0],bins=bin_array)\n",
      "    a = ax1.hist(whole_data[item][whole_labels.values==1],bins=bin_array,histtype=u'step',label='Blends');\n",
      "    b = ax1.hist(whole_data[item][whole_labels.values==0],bins=bin_array,histtype=u'step',label='Non-Blends');\n",
      "    plt.legend(prop={'size':prop_fs},loc=loc)\n",
      "    plt.title(title,fontsize=fs); \n",
      "    plt.xlabel(xlabel,fontsize=fs); plt.ylabel('Occurrence',fontsize=fs)\n",
      "    \n",
      "    if gmm != True:\n",
      "        return ax1\n",
      "    \n",
      "    # GMM\n",
      "    from sklearn import mixture\n",
      "    g = mixture.GMM(n_components=2)\n",
      "    y = np.array([whole_data[item].values])\n",
      "    g.fit(y.T)\n",
      "    c = ax1.plot(bin_array,(max(blend_hist[0]))*gaussian(bin_array, g.means[1], g.covars[1][0][0]),'r--',label='GMM Component 1')\n",
      "    d = ax1.plot(bin_array,(max(unblend_hist[0]))*gaussian(bin_array, g.means[0], g.covars[0][0][0]),'k--',label='GMM Component 2')\n",
      "    plt.legend(prop={'size':prop_fs},loc=loc)\n",
      "    \n",
      "    return ax1\n",
      "def gaussian(x, mu, var):\n",
      "    return np.exp(-np.power(x - mu, 2.) /( 2 * var))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(15,10))\n",
      "fig.set_facecolor('white')\n",
      "gs = Gridspec.GridSpec(1,2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Construct plots to visualize the data distributions using the whole dataset\n",
      "# Gini coefficient\n",
      "fs = 18\n",
      "prop_fs = 12\n",
      "item = 'gini_c'\n",
      "title = 'Gini Coefficient Distribution'\n",
      "xlabel = '$G$'\n",
      "bin_factor = 0.01\n",
      "loc = 2\n",
      "pos1=0;pos2=0\n",
      "ax1 = histogram(whole_data,fs,prop_fs,item,title,xlabel,bin_factor,loc,pos1,pos2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Asymmetry coefficient\n",
      "fs = 18\n",
      "prop_fs = 14\n",
      "item = 'a_c'\n",
      "title = 'Asymmetry Coefficient Distribution'\n",
      "xlabel = '$A_{abs}$'\n",
      "bin_factor = 0.01\n",
      "loc = 1\n",
      "pos1=0;pos2=1;\n",
      "ax2 = histogram(whole_data,fs,prop_fs,item,title,xlabel,bin_factor,loc,pos1,pos2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(15,10))\n",
      "fig.set_facecolor('white')\n",
      "gs = Gridspec.GridSpec(1,2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# m_20 statistic\n",
      "fs = 18\n",
      "prop_fs = 14\n",
      "item = 'm_20'\n",
      "title = '$M_{20}$ Statistic Distribution'\n",
      "xlabel = '$M_{20}$ Statistic'\n",
      "bin_factor = 0.05\n",
      "loc=1\n",
      "pos1=0;pos2=0;\n",
      "ax1 = histogram(whole_data,fs,prop_fs,item,title,xlabel,bin_factor,loc,pos1,pos2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# \n",
      "fs = 18\n",
      "prop_fs = 14\n",
      "item = 'n_centroids'\n",
      "title = 'Centroid Estimation'\n",
      "xlabel = 'Centroid Number'\n",
      "bin_factor = 0.5\n",
      "loc=1\n",
      "pos1=0;pos2=1;\n",
      "ax2 = histogram(whole_data,fs,prop_fs,item,title,xlabel,bin_factor,loc,pos1,pos2,gmm=False)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}